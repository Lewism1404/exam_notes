1 a

```python
x0 = w[1:] - y[:-1]
x1 = np.cumsum(x0)
x2 = np.diff(x0)
x0 = x0[1:]
```

1 b i

loss function

L(u) = ||sim(u) - w||

our parameters are just the target temperature

1 b ii



1 b iii

disadvantage of GD: since sum(u) is non convex, gradient descent may converge to a local minima instead of a global minima.
advantage of GD: it will converge to some value. faster

disadvantage of random search: it will never converge on a point.
advantage of random search: you can escape local minima.

1 b iv

we must apply a inequality constraint, this will reject parameters that do not follow the constraint
function

1 c i

no label on right axis
colours are not preceptually linear and monotonic
the guides dont explain what they represent
no units on x axis and y axis
no uncertainty shown
no title
overlapping geoms

1 c ii

should be perceptually linear
should be monotonic brightness
signed

1 c iii

you could have a signed scalar around the mean so that the values seem better on average.
you could make the colour map not vary in colour much, so it seems like the data doesnt vary much.

2 a i

2 a ii

since a and c, b and d have different magnitudes, their addition will result in a magnutude error,
so they will be more unstable, so 2 will be more unstable, and 1 will be more stable.

2 b i

byte offset = i * s0, j * s1

The strides depend on the dtype of the array, but we do not need them here

2 b ii

strides are (-4, 16)

2 d i

Matrix multiplication is not commutative, so this is not neccisarily true.

2 d ii

If all matricies are diagonal, that is for A = UZV^T, we have U and V are the identity.

2 d iii

cosin distance

2 d iv

d

3 a

```py
product_11 = sales[sales[:,2] == 11]

conversion_rate = product_11[:,5] / product_11[:,4]

revenue_per_impression = (product_11[:, 3] * product_11[:, 5]) /  product_11[:,4]
```

3 b

I would split the two datasets up into 2 facets.

Both facets would have week as the x axis, making sure to include all weeks in the graph, not cutting any out.

On the y axis for the conversion_rate i would have a label on the axis, with values and indicating the units (purchases per impression)

i would have geoms on both graphs as in markers, both graphs would also have lines connecting each point.

3 c i

it is discrete as it depends on the week which is from a fixed set of values



2022

1 a i

60 * 60 seconds in an hour
40 records per second

3600 * 40 records
36000 * 4

 24000 +
120000
144000

shape is (144000, 3)


1 a ii

y = arr.reshape((3600, 40, 3))

means = np.mean(y[:, :, 0], axis=1)

1 b i

(2, 2)

1 b ii

they are just scalings of each other

1 b iii

```py
def get_scaling(A, B, path):
    cov = np.cov(path)
    os = B @ A @ cov
    eigvals, eigvecs = np.linalg.eigh(os)
    return 1000 / (np.sqrt(np.max(eigvals)) * 2)
```
1 c i

there are no captions or titles
there are no axes and no labels on the axes
there is no coordinate system
the colours are hard to interpret, they are not preceptually linear
there is no guide to tell the six droes apartYou are building a model of crop growth. This depends on soil moisture content. Moisture
content varies, but has regular periodic variations due to daily and annual weather cycles.
(a) (i) Describe a signal processing approach that could determine the strength of these cycles
given a time series of regularly sampled soil moisture measurements.
the marks are big for the points where the drones have been

1 c ii

This is layered, there is a layer showing the fertillity levels and there is a layer showing the paths of the drones.

2 a i

we should sample the data, apply a fast fourier transform

2 a ii

we could use a moving average, to smooth data, this works by using a sliding window across the data and for each sliding window
we take the average of the data and set centre value to the average.

2 b i

3 a i

stochastic hill climbing

it may never reach the areas where it is best to find.
choosing a step size may be hard
it an struggle in areas which have similar quality.
can be slow


2022

1 a i

you can perform vectories operations on the data, which is faster. Points in R4

1 a ii

L_(-inf)

1 a iii

data could vary a lot in other areas, like depth, but you wont track that in the min norm

1 b i

```py
np.reshape(X[np.argsort(np.linalg.norm(X - v, ord=2, axis=0))], (8, 8, N))
```

1 b ii

gets the largest minumum frequency from the 4th frequency of all frequency readings

1 c i

they should use dft, since the data is measure at regularly spaced intervals in time.
You would get each function and get the frequency from that

1 c ii

No, linear filters can only modify existing frequencies, not introduce new ones.

1 c iii

1 d

linear, because all data is close in magnitude
semilog-y
symlog-y

2 a

the stride variable has not been defined,
indexing strides at s "strides[s]" will be undefined and will raise an error
it is not using vecoriesd operations for addition.
they are not using vector operations for multiplication, they could have used cumprod

2 b


2 c i

get the SVD of A and A|y, then get the diagonal matrix Sigma, the rank is the number of non zero singular values, so the number of non zero values on the diagonal of Sigma

```py
def has_solution(A, y):
    A_U, A_S, A_V = np.linalg.svd(A)
    Ay_U, Ay_S, Ay_V = np.linalg.svd(np.concat(A, y))
    return np.count_nonzero(np.diag(A_S)) == np.count_nonzero(np.diag(Ay_S))
```

2 d i

discrete fourier transform
returns complex values, one per frequency component
i ranges over frequency components


2 d ii

```py
def q(x):
    yr = x *


```